{"cells":[{"cell_type":"markdown","id":"9ea2b39e","metadata":{"id":"9ea2b39e"},"source":["### Creamos el Dataframe"]},{"cell_type":"code","execution_count":null,"id":"7db55689","metadata":{"id":"7db55689"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","ruta = '/home/luisa/Downloads/df_estados_bank.csv'\n","\n","\n","df = pd.read_csv(ruta, sep=',')\n","\n"]},{"cell_type":"markdown","id":"294d6e60","metadata":{"id":"294d6e60"},"source":["### Creamos una nueva copia y hacemos el EDA"]},{"cell_type":"code","execution_count":null,"id":"ce0ff687","metadata":{"id":"ce0ff687","outputId":"6fdd3275-7d4f-4eb1-beff-ba2dac24e8db"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditScore</th>\n","      <th>Gender</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>Age</th>\n","      <th>Geography</th>\n","      <th>EstimatedSalary</th>\n","      <th>Tenure</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>619</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>42.0</td>\n","      <td>0</td>\n","      <td>101348.88</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>608</td>\n","      <td>0</td>\n","      <td>83807.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>41.0</td>\n","      <td>1</td>\n","      <td>112542.58</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>502</td>\n","      <td>0</td>\n","      <td>159660.80</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>42.0</td>\n","      <td>0</td>\n","      <td>113931.57</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>699</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.0</td>\n","      <td>0</td>\n","      <td>93826.63</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>850</td>\n","      <td>0</td>\n","      <td>125510.82</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>43.0</td>\n","      <td>1</td>\n","      <td>79084.10</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>771</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>39.0</td>\n","      <td>0</td>\n","      <td>96270.64</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>516</td>\n","      <td>1</td>\n","      <td>57369.61</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>101699.77</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>709</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>37.0</td>\n","      <td>0</td>\n","      <td>42085.58</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>772</td>\n","      <td>1</td>\n","      <td>75075.31</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>42.0</td>\n","      <td>2</td>\n","      <td>92888.52</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>792</td>\n","      <td>0</td>\n","      <td>130142.79</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>28.0</td>\n","      <td>0</td>\n","      <td>38190.78</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 10 columns</p>\n","</div>"],"text/plain":["      CreditScore  Gender    Balance  NumOfProducts  HasCrCard  \\\n","0             619       0       0.00              1          1   \n","1             608       0   83807.86              1          0   \n","2             502       0  159660.80              3          1   \n","3             699       0       0.00              2          0   \n","4             850       0  125510.82              1          1   \n","...           ...     ...        ...            ...        ...   \n","9995          771       1       0.00              2          1   \n","9996          516       1   57369.61              1          1   \n","9997          709       0       0.00              1          0   \n","9998          772       1   75075.31              2          1   \n","9999          792       0  130142.79              1          1   \n","\n","      IsActiveMember   Age  Geography  EstimatedSalary  Tenure  \n","0                  1  42.0          0        101348.88       2  \n","1                  1  41.0          1        112542.58       1  \n","2                  0  42.0          0        113931.57       8  \n","3                  0  39.0          0         93826.63       1  \n","4                  1  43.0          1         79084.10       2  \n","...              ...   ...        ...              ...     ...  \n","9995               0  39.0          0         96270.64       5  \n","9996               1  35.0          0        101699.77      10  \n","9997               1  37.0          0         42085.58       7  \n","9998               0  42.0          2         92888.52       3  \n","9999               0  28.0          0         38190.78       4  \n","\n","[10000 rows x 10 columns]"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["dfN = df.copy()\n","\n","# Mapeo para la variable Género\n","\n","mapeo_genero = {'Female': 0, 'Male': 1}\n","dfN['Gender'] = dfN['Gender'].map(mapeo_genero)\n","dfN\n","\n","# Mapeo para la variable Estado\n","mapeo_estado = {'Texas': 0, 'California': 1, 'Alabama': 2}\n","dfN['Geography'] = dfN['Geography'].map(mapeo_estado) # Imprimir el DataFrame actualizado print(dfN7)\n","dfN\n","\n","# *** Ya que tenemos claro que NO nos sirve de nada esa columna, la borramos y creamos un nuevo 'DFN3'\n","dfN = dfN.drop(['RowNumber'], axis=1)\n","dfN = dfN.drop(['Unnamed: 0'], axis=1)\n","dfN\n","\n","\n","# Quitamos la columna de la Identificación porque creemos que no nos aporta nada para la predicción (CustomerId)\n","dfN = dfN.drop(['CustomerId'], axis=1)\n","dfN\n","\n","# Quitamos la columna del apellido (Surname)\n","dfN = dfN.drop(['Surname'], axis=1)\n","dfN\n","\n","# Reemplazo los nulos con la mediana\n","dfN['Age'].fillna(dfN['Age'].median(), inplace=True)\n","dfN['EstimatedSalary'].fillna(dfN['EstimatedSalary'].median(), inplace=True)\n","dfN\n","\n","#Mapeo , se eliminan outliers\n","columnas_outliers = [\"Age\",]\n","\n","for columna in columnas_outliers:\n","    Q1 = np.quantile(dfN[columna], 0.25)\n","    Q3 = np.quantile(dfN[columna], 0.75)\n","    IQR = Q3 - Q1\n","\n","    lower_limit = Q1 - 1.5 * IQR\n","    upper_limit = Q3 + 1.5 * IQR\n","\n","dfN[columna] = np.clip(dfN[columna], lower_limit, upper_limit)\n","\n","#En este caso eliminaremos la variable \"Exited\" para probar nuestro df con las variables independientes\n","#Creamos la variable target a parte\n","dfN_Target = dfN['Exited']\n","dfN = dfN.drop('Exited', axis=1)\n","dfN"]},{"cell_type":"markdown","id":"042f39c7","metadata":{"id":"042f39c7"},"source":["### Usamos Regresión logística"]},{"cell_type":"markdown","id":"175fcd59","metadata":{"id":"175fcd59"},"source":["#### Primero haremos una regresión logística simple\n","\n","* La elegimos ya que se utiliza para predecir una variable categórica binaria como es nuestro caso.\n","* Es una extensión de la regresión lineal, en la cual se utiliza la función logística para modelar\n","    la probabilidad de que un evento ocurra (probabilidad de clase 1) a partir de una combinación\n","    lineal de las variables independientes\n"]},{"cell_type":"code","execution_count":null,"id":"2c0a0f72","metadata":{"id":"2c0a0f72","outputId":"fbb8d2fb-4e16-4b68-88f9-f3220dfba7a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión: 84.200000%\n","[[1684    0]\n"," [ 316    0]]\n"]}],"source":["\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix\n","\n","\n","# importamos train_test_split que es para separar los datos\n","\n","from sklearn.model_selection import train_test_split\n","\n","X = dfN # ==> nuestro dataframe\n","y = dfN_Target # ==> Target\n","\n","#Con test_size definimos el porcentaje de datos a entrenar\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n","\n","\n","#Definimos el modelo de regresión logística\n","\n","clf = LogisticRegression()\n","\n","#Entrenamos el modelo con el conjunto de entrenamiento\n","\n","clf.fit(X_train, y_train)\n","\n","#Realizamos las predicciones con el conjunto de prueba\n","\n","y_pred = clf.predict(X_test)\n","\n","#Imprimimos la precisión del modelo\n","\n","print(\"Precisión: {:2f}%\".format(clf.score(X_test,y_test)*100))\n","\n","\n","# Haremos una matriz de confusión para calcular las metricas respectivas y ver el rendimiento del modelo\n","\n","matriz = confusion_matrix(y_test , y_pred)\n","print(matriz)"]},{"cell_type":"markdown","id":"9f46add7","metadata":{"id":"9f46add7"},"source":["#### Luego haremos una regresión logística con StandardScaler"]},{"cell_type":"markdown","id":"bcb20880","metadata":{"id":"bcb20880"},"source":[" *Al estandarizar las características, todas tendrán una media de cero y una desviación estándar de uno, lo que asegura que las características estén en la misma escala y evita que una característica con una magnitud mayor tenga un impacto dominante.*"]},{"cell_type":"code","execution_count":null,"id":"f1d9b489","metadata":{"id":"f1d9b489","outputId":"440f5bb5-26fb-4663-a845-b358e4d46c2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión: 84.300000%\n","[[1636   27]\n"," [ 287   50]]\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","\n","\n","X = dfN # ==> nuestro dataframe\n","y = dfN_Target # ==> nuesta target\n","\n","#Con test_size definimos el porcentaje de datos a entrenar\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n","\n","\n","# escalamos los  datos para llevarlos a una escala de mismas magnitudes\n","escalar = StandardScaler()\n","\n","#con fit_transform nos devuelve los datos ya escalados\n","\n","X_train=escalar.fit_transform(X_train)\n","X_test=escalar.fit_transform(X_test)\n","\n","#Definimos el modelo de regresión logística\n","\n","clf = LogisticRegression ()\n","\n","#Entrenamos el modelo\n","\n","clf.fit(X_train, y_train)\n","\n","#Realizamos la predicción con la instrucción \"Predict\"\n","\n","y_pred = clf.predict(X_test)\n","\n","#Imprimimos la precisión del modelo\n","\n","print(\"Precisión: {:2f}%\".format(clf.score(X_test,y_test)*100))\n","\n","# Haremos una matriz de confusión para calcular las metricas respectivas y ver el rendimiento del modelo\n","\n","matriz = confusion_matrix(y_test , y_pred)\n","print(matriz)\n"]},{"cell_type":"markdown","id":"fc1525de","metadata":{"id":"fc1525de"},"source":["### Random Forest"]},{"cell_type":"markdown","id":"e50cb0c1","metadata":{"id":"e50cb0c1"},"source":["* Es una técnica que combina múltiples árboles de decisión individuales para obtener predicciones más precisas y robustas.\n","* Random Forest es efectivo en conjuntos de datos complejos que contienen múltiples características y relaciones no lineales como es nuestro caso en nuestro dataset."]},{"cell_type":"code","execution_count":null,"id":"91edeb7a","metadata":{"id":"91edeb7a","outputId":"c46e065e-afd5-406f-b140-cec2d8747de5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Puntuación: 0.8407\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","#Creamos los datos de entrenamiento\n","\n","X, y = dfN, dfN_Target\n","\n","#Creamos una instancia con RandomForestClassifier\n","#n_estimators = número de árboles\n","\n","clf = RandomForestClassifier(n_estimators = 100, max_depth=3)\n","\n","#Entrenamos el modelo con los datos de entrenamiento\n","\n","clf = clf.fit(X,y)\n","\n","#Mostramos la puntuación del modelo\n","\n","print(\"Puntuación:\", clf.score(X,y))\n","\n","#Realizamos una predicción\n","\n","# La puntuación va del 0 al 1"]},{"cell_type":"code","execution_count":null,"id":"5a81f324","metadata":{"id":"5a81f324"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fdea4de8","metadata":{"id":"fdea4de8"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}